# ğŸ•¸ï¸ Web Scraping Projects

![Python](https://img.shields.io/badge/python-3.11+-blue.svg)
![uv](https://img.shields.io/badge/uv-enabled-purple.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)

A collection of professional, production-grade web scraping tools built with Python 3.11. This repository demonstrates best practices in web scraping, including robust error handling, anti-detection techniques, and structured data extraction.

---

## âœ¨ Features

*   **Modular Architecture**: Separate projects for E-commerce, Job Portals, News Aggregation, and Real Estate.
*   **Modern Tooling**: Built with `uv` for lightning-fast dependency management (compatible with `pip`).
*   **Cross-Platform**: Full support for Windows, macOS, and Linux (including Kali/Arch).
*   **Robust Logging**: Detailed logs for every operation to aid debugging.
*   **Data Export**: Automatically saves data in CSV and JSON formats.

---

## ï¿½ Folder Structure

```tree
web_scraping_projects/
â”œâ”€â”€ ecommerce_price_tracker/    # Tracks product prices on Amazon/Flipkart
â”‚   â”œâ”€â”€ tracker.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ tracker.log
â”œâ”€â”€ job_portal_scraper/         # Scrapes job listings
â”‚   â”œâ”€â”€ scraper.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ scraper.log
â”œâ”€â”€ news_headline_aggregator/   # Aggregates news headlines
â”‚   â”œâ”€â”€ aggregator.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ aggregator.log
â”œâ”€â”€ real_estate_crawler/        # Crawls real estate listings
â”‚   â”œâ”€â”€ crawler.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ crawler.log
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

---

## ğŸ› ï¸ Requirements

*   **Python 3.11+**
*   **uv** (Recommended) or **pip**
*   **Git**

---

## ğŸ“¥ Installation

### ğŸªŸ Windows

1.  **Install Python**:
    Download and install Python 3.11+ from the [official website](https://www.python.org/downloads/) or via `uv` (see below). Ensure you check **"Add Python to PATH"**.

2.  **Install `uv` (PowerShell)**:
    ```powershell
    powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
    ```

3.  **Clone the Repository**:
    ```powershell
    git clone <your-repo-url>
    cd web_scraping_projects
    ```

### ğŸ macOS

1.  **Install Homebrew** (if not installed):
    ```bash
    /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
    ```

2.  **Install Python & uv**:
    ```bash
    brew install python@3.11 uv
    ```

3.  **Clone the Repository**:
    ```bash
    git clone <your-repo-url>
    cd web_scraping_projects
    ```

### ğŸ§ Linux (Ubuntu/Debian + Arch + Kali)

#### Ubuntu / Debian / Kali
1.  **Update & Install Basics**:
    ```bash
    sudo apt update && sudo apt install -y python3 python3-pip git curl
    ```
    *(Note for Kali Users: Ensure your repositories are updated. You may need to use `venv` for all pip operations due to PEP 668).*

2.  **Install `uv`**:
    ```bash
    curl -LsSf https://astral.sh/uv/install.sh | sh
    ```

#### Arch Linux
1.  **Install Dependencies**:
    ```bash
    sudo pacman -S python python-pip uv git
    ```

---

## âš™ï¸ Environment Setup

We recommend using **uv** for the fastest and most reliable experience.

### Option A: Using `uv` (Recommended)

1.  **Create Virtual Environment**:
    ```bash
    uv venv
    ```
    *Windows users: This logs strictly to `v` folder by default in some configs, or standard `.venv`.*

2.  **Activate Environment**:
    *   **Linux/macOS**: `source .venv/bin/activate`
    *   **Windows**: `.venv\Scripts\activate`

3.  **Install Dependencies**:
    ```bash
    uv pip install -r requirements.txt
    ```
    *Or to sync all dependencies at once if a lockfile existed:*
    ```bash
    uv sync
    ```

### Option B: Using Standard `pip`

1.  **Create Virtual Environment**:
    ```bash
    python3 -m venv venv
    ```

2.  **Activate Environment**:
    *   **Linux/macOS**: `source venv/bin/activate`
    *   **Windows**: `venv\Scripts\activate`

3.  **Install Dependencies**:
    ```bash
    pip install -r requirements.txt
    ```

---

## ğŸš€ Running the Project

Navigate to the specific project folder you want to run.

**Example: E-commerce Price Tracker**

```bash
cd ecommerce_price_tracker
```

**Using `uv`:**
```bash
uv run tracker.py
```

**Using Standard Python:**
```bash
# Ensure venv is active
python tracker.py
```

---

## ğŸ“¦ Updating Dependencies

**With `uv`:**
```bash
uv pip compile requirements.in -o requirements.txt  # If using .in files
# Or simply update installed packages
uv pip install -U -r requirements.txt
```

**With `pip`:**
```bash
pip install -U -r requirements.txt
```

---

## ğŸ“ Logs & Troubleshooting

### Logs Location
Each sub-project writes logs to its own directory:
*   `ecommerce_price_tracker/tracker.log`
*   `job_portal_scraper/scraper.log`
*   etc.

### Troubleshooting

*   **"Module not found"**: Ensure your virtual environment is activated (`source .venv/bin/activate`).
*   **Permission Denied (Linux/Mac)**: You might need to `chmod +x` scripts or check folder permissions.
*   **Kali Linux**: If `pip` fails with "externally-managed-environment", use a virtual environment (`uv venv` or `python3 -m venv .venv`).

---

## ğŸ’¡ Recommendations

### Recommended Tools
*   **VS Code Extensions**:
    *   Python (Microsoft)
    *   Pylance
    *   Ruff (for linting)
*   **CLI Tools**:
    *   `jq` (for processing JSON output on CLI)
    *   `httpie` (alternative to curl)

### Security Notes
*   Never commit API keys to GitHub.
*   Use environment variables for sensitive data.

### Performance
*   **Python Version Manager**: `uv` is recommended over `pyenv` and `conda` for speed.
*   **Libraries**: We use standard, high-performance libraries. Consider `aiohttp` or `playwright` for more complex async scraping needs in the future.

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
